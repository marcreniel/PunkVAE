{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders (VAEs) for CryptoPunks Dataset\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Jupyter notebook explores the application of Variational Autoencoders (VAEs) to the CryptoPunks dataset. CryptoPunks are unique digital collectibles on the Ethereum blockchain, consisting of 10,000 algorithmically generated characters with distinct attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on Macbook Pro with M1 chip, using Metal Performance Shaders\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"huggingnft/cryptopunks\", split=\"train\")\n",
    "\n",
    "# Define split ratio\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Create the train-test split\n",
    "split_dataset = dataset.train_test_split(test_size=1 - train_ratio)\n",
    "\n",
    "# Access the train and test splits\n",
    "train_data = split_dataset['train']\n",
    "test_data = split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert image to tensor\n",
    "def transform_func(img):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  # Transform image to 128x128\n",
    "        transforms.PILToTensor(),  # Convert PIL Image to PyTorch tensor\n",
    "        transforms.ConvertImageDtype(torch.float)   # Convert PIL Image to float\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "# Define class for Cryptopunks dataset\n",
    "class PunkDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "        self.transform = transform_func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data_list[idx][\"image\"]\n",
    "        if isinstance(image, Image.Image):\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "# Initialize the Punk dataset for training and test sets\n",
    "train_dataset = PunkDataset(train_data)\n",
    "test_dataset = PunkDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the dataset sizes that they are 80% and 20% of the total dataset\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images in a grid\n",
    "def plot_images_from_loader(data_loader, title, num_images=8):\n",
    "    \"\"\"Display a set of images from the DataLoader in a grid.\"\"\"\n",
    "    # Get a batch of images\n",
    "    images = next(iter(data_loader))\n",
    "    \n",
    "    # Create a grid from the images\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(32, 3))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    # Plot each image\n",
    "    for i in range(num_images):\n",
    "        ax = axes[i]\n",
    "        image = images[i].permute(1, 2, 0).numpy()  # Convert tensor to NumPy array\n",
    "        ax.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Display images from training and test loaders\n",
    "plot_images_from_loader(train_loader, title=\"Training Images\")\n",
    "plot_images_from_loader(test_loader, title=\"Test Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding network based on a simple forward feed neural network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc_mean = nn.Linear(512, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mean = self.fc_mean(x) # Mean of the latent space\n",
    "        logvar = self.fc_logvar(x) # Log variance of the latent space\n",
    "        return mean, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding network based on a simple forward feed neural network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 128 * 16 * 16)\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.conv2 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.conv3 = nn.ConvTranspose2d(32, 3, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = nn.ReLU()(self.fc2(x))\n",
    "        x = x.view(x.size(0), 128, 16, 16)\n",
    "        \n",
    "        x = nn.ReLU()(self.conv1(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = nn.ReLU()(self.conv2(x))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = torch.sigmoid(self.conv3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Autoencoder Model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "\n",
    "    # Reparameterization trick\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VAE\n",
    "latent_dim = 128\n",
    "vae = VAE(latent_dim)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-4)\n",
    "\n",
    "# Define the loss function for the VAE, which is a combination of Binary Cross Entropy (BCE) and Kullback-Leibler Divergence (KLD)\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE, KLD\n",
    "\n",
    "# Training loop\n",
    "def train_vae(dataloader, model, optimizer, num_epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_bce = 0.0\n",
    "        running_kld = 0.0\n",
    "        for images in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            recon_images, mu, logvar = model(images)\n",
    "            bce_loss, kl_loss = vae_loss(recon_images, images, mu, logvar)\n",
    "            loss = bce_loss + kl_loss\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_bce += bce_loss.item()\n",
    "            running_kld += kl_loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_bce = running_bce / len(dataloader.dataset)\n",
    "        epoch_kld = running_kld / len(dataloader.dataset)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Total Loss: {epoch_loss:.4f}, \"\n",
    "              f\"BCE Loss: {epoch_bce:.4f}, KL Loss: {epoch_kld:.4f}\")\n",
    "\n",
    "# Train the VAE\n",
    "train_vae(train_loader, vae, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display original and reconstructed images\n",
    "def plot_vae_results(data_loader, model, num_images=8):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images = next(iter(data_loader))\n",
    "        recon_images, _, _ = model(images)\n",
    "    \n",
    "    # Convert images and reconstructed images to NumPy arrays for visualization\n",
    "    images = images.cpu().numpy()\n",
    "    recon_images = recon_images.cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_images, figsize=(15, 4))\n",
    "    fig.suptitle(\"Original and Reconstructed Images (VAE)\")\n",
    "    \n",
    "    # Display original images\n",
    "    for i in range(num_images):\n",
    "        ax = axes[0, i]\n",
    "        img = images[i].transpose(1, 2, 0)\n",
    "        ax.imshow(np.clip(img, 0, 1))\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Display reconstructed images\n",
    "    for i in range(num_images):\n",
    "        ax = axes[1, i]\n",
    "        img = recon_images[i].transpose(1, 2, 0)\n",
    "        ax.imshow(np.clip(img, 0, 1))\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display images\n",
    "plot_vae_results(test_loader, vae, num_images=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state dictionary\n",
    "torch.save(vae.state_dict(), 'autoencoder.pth')\n",
    "\n",
    "# Save the entire model\n",
    "torch.save(vae, 'autoencoder_complete.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
